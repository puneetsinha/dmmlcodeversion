"""
DMML Data Pipeline - Airflow DAG Implementation
Students: 2024ab05134, 2024aa05664

The word "Orchestrate" in data science / ML / cloud computing contexts means:

Coordinating and automating multiple tasks, processes, or services so they work
together smoothly as one system.

It's like being a conductor of an orchestra â€” ensuring each instrument (data pipeline,
model training, deployment, monitoring) plays at the right time in harmony.
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
import logging

# Define pipeline functions
def data_ingestion():
    """Execute data ingestion from multiple sources."""
    logging.info("Data ingestion started.")
    # Simulate data ingestion process
    import time
    time.sleep(2)
    logging.info("Data ingestion completed.")
    
def raw_data_storage():
    """Store raw data in data lake."""
    logging.info("Raw data storage started.")
    # Simulate data storage process
    import time
    time.sleep(1)
    logging.info("Raw data storage completed.")

def data_validation():
    """Validate data quality and consistency."""
    logging.info("Data validation started.")
    # Simulate data validation process
    import time
    time.sleep(1)
    logging.info("Data validation completed.")

def data_preparation():
    """Prepare and clean data for processing."""
    logging.info("Data preparation started.")
    # Simulate data preparation process
    import time
    time.sleep(2)
    logging.info("Data preparation completed.")

def data_transformation():
    """Transform and engineer features."""
    logging.info("Data transformation started.")
    # Simulate data transformation process
    import time
    time.sleep(3)
    logging.info("Data transformation completed.")

def feature_store():
    """Manage and store engineered features."""
    logging.info("Feature store tasks started.")
    # Simulate feature store operations
    import time
    time.sleep(1)
    logging.info("Feature store tasks completed.")

def data_versioning():
    """Version control for data and models."""
    logging.info("Data versioning started.")
    # Simulate data versioning process
    import time
    time.sleep(1)
    logging.info("Data versioning completed.")

def model_building():
    """Train and evaluate ML models."""
    logging.info("Model building started.")
    # Simulate model training process
    import time
    time.sleep(4)
    logging.info("Model building completed.")

# Default arguments for the DAG
default_args = {
    'owner': 'airflow',
    'start_date': datetime.now() - timedelta(days=1),
    'retries': 1,
}

# Create the DAG
with DAG(
    'dmml_data_pipeline',
    default_args=default_args,
    description='Orchestrates the end-to-end data pipeline for customer churn prediction',
    schedule_interval='*/10 * * * *',
    catchup=False,
) as dag:

    # Create tasks using PythonOperator
    task_ingestion = PythonOperator(
        task_id='data_ingestion',
        python_callable=data_ingestion
    )

    task_storage = PythonOperator(
        task_id='raw_data_storage',
        python_callable=raw_data_storage
    )

    task_validation = PythonOperator(
        task_id='data_validation',
        python_callable=data_validation
    )

    task_preparation = PythonOperator(
        task_id='data_preparation',
        python_callable=data_preparation
    )

    task_transformation = PythonOperator(
        task_id='data_transformation',
        python_callable=data_transformation
    )

    task_feature_store = PythonOperator(
        task_id='feature_store',
        python_callable=feature_store
    )

    task_versioning = PythonOperator(
        task_id='data_versioning',
        python_callable=data_versioning
    )

    task_model_building = PythonOperator(
        task_id='model_building',
        python_callable=model_building
    )

    # Define task dependencies (pipeline flow)
    task_ingestion >> task_storage >> task_validation >> task_preparation
    task_preparation >> task_transformation >> task_feature_store
    task_feature_store >> task_versioning >> task_model_building

# Pipeline Summary
print("""
DMML PIPELINE ORCHESTRATION SUMMARY
===================================
Students: 2024ab05134, 2024aa05664

Pipeline Components:
1. Data Ingestion       -> Collect data from multiple sources
2. Raw Data Storage     -> Store in data lake with partitioning
3. Data Validation      -> Quality checks and consistency validation
4. Data Preparation     -> Cleaning and preprocessing
5. Data Transformation  -> Feature engineering and scaling
6. Feature Store        -> Centralized feature management
7. Data Versioning      -> Version control for data and models
8. Model Building       -> Train and evaluate ML models

Orchestration Features:
- Automated task scheduling (every 10 minutes)
- Dependency management with proper sequencing
- Error handling with retry mechanism
- Comprehensive logging and monitoring
- Scalable and maintainable architecture

Total Pipeline Steps: 8
Execution Strategy: Sequential with dependencies
Monitoring: Airflow UI dashboard
Deployment: Production-ready with error handling
""")
